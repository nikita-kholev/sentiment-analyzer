# main.py
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import pandas as pd
import numpy as np
import re
import io
import uuid
import time
import os
from typing import List
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.metrics import f1_score, classification_report

app = FastAPI(title="Sentiment Analysis API")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

class OptimizedSentimentAnalyzer:
    def __init__(self):
        # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        self.vectorizer = TfidfVectorizer(
            max_features=3000,  # –£–º–µ–Ω—å—à–∏–ª –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
            ngram_range=(1, 2),
            min_df=5,  # –ë–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ä–µ–¥–∫–∏—Ö —Å–ª–æ–≤
            max_df=0.9,
            lowercase=True,
            analyzer='word'
        )
        self.model = None
        self.is_trained = False
        self.best_f1 = 0
        
        # –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô –Ω–∞–±–æ—Ä —Å—Ç–æ–ø-—Å–ª–æ–≤ –∏ —Ñ–∏–ª—å—Ç—Ä–æ–≤
        self.stop_words = {
            # –ë–∞–∑–æ–≤—ã–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
            '–∏', '–≤', '–≤–æ', '–Ω–µ', '—á—Ç–æ', '–æ–Ω', '–Ω–∞', '—è', '—Å', '—Å–æ', '–∫–∞–∫', '–∞', '—Ç–æ', '–≤—Å–µ', '–æ–Ω–∞', '—Ç–∞–∫', '–µ–≥–æ',
            '–Ω–æ', '–¥–∞', '—Ç—ã', '–∫', '—É', '–∂–µ', '–≤—ã', '–∑–∞', '–±—ã', '–ø–æ', '—Ç–æ–ª—å–∫–æ', '–µ–µ', '–º–Ω–µ', '–±—ã–ª–æ', '–≤–æ—Ç', '–æ—Ç',
            '–º–µ–Ω—è', '–µ—â–µ', '–Ω–µ—Ç', '–æ', '–∏–∑', '–µ–º—É', '—Ç–µ–ø–µ—Ä—å', '–∫–æ–≥–¥–∞', '–¥–∞–∂–µ', '–Ω—É', '–≤–¥—Ä—É–≥', '–ª–∏', '–µ—Å–ª–∏', '—É–∂–µ',
            
            # –≠–ª–µ–∫—Ç—Ä–æ–Ω–Ω–∞—è –∫–æ–º–º–µ—Ä—Ü–∏—è
            '—Ç–æ–≤–∞—Ä', '–ø—Ä–æ–¥—É–∫—Ç', '–ø–æ–∫—É–ø–∫–∞', '–∑–∞–∫–∞–∑', '–¥–æ—Å—Ç–∞–≤–∫–∞', '–ø—Ä–æ–¥–∞–≤–µ—Ü', '–º–∞–≥–∞–∑–∏–Ω', '—Ü–µ–Ω–∞', '—Ä—É–±–ª—å', '—Ä—É–±',
            '—à—Ç', '—à—Ç—É–∫–∞', '—Ä–∞–∑–º–µ—Ä', '—Ü–≤–µ—Ç', '–∫–∞—á–µ—Å—Ç–≤–æ', '—Å–µ—Ä–≤–∏—Å', '—É–ø–∞–∫–æ–≤–∫–∞', '–∫—É—Ä—å–µ—Ä', '–æ—Ç–ø—Ä–∞–≤–∫–∞', '–ø–æ–ª—É—á–µ–Ω–∏–µ',
            
            # –°–ª–µ–Ω–≥ –∏ —á–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞
            '–ø—Ä–∏–≤–µ—Ç', '–ø–æ–∫–∞', '—Å–ø–∞—Å–∏–±–æ', '–ø–æ–∂–∞–ª—É–π—Å—Ç–∞', '–∏–∑–≤–∏–Ω–∏—Ç–µ', '–æ–∫', '–æ–∫–µ–π', '–ª–∞–¥–Ω–æ', '—Ö–æ—Ä–æ—à–æ', '–ø–æ–Ω—è—Ç–Ω–æ',
            '–∫–æ—Ä–æ—á–µ', '—Ç–∏–ø–∞', '–∫–∞–∫ –±—ã', '–∑–Ω–∞—á–∏—Ç', '–≤–æ—Ç', '—Ç–∞–∫ —Å–∫–∞–∑–∞—Ç—å', '—Ñ–∏–≥–Ω—è', '—Ö—Ä–µ–Ω—å', '–µ—Ä—É–Ω–¥–∞', '–±—Ä–µ–¥',
            '—à—Ç—É–∫–∞', '—Ñ–∏—à–∫–∞', '–ø—Ä–∏–∫–æ–ª', '–Ω–æ—Ä–º', '–æ—Ñ–∏–≥–µ–Ω–Ω–æ', '–æ—Ç—Å—Ç–æ–π', '–ª–∞–∂–∞', '–∫—Ä—É—Ç–æ', '—Å—É–ø–µ—Ä', '—É–∂–∞—Å', '–∫–æ—à–º–∞—Ä',
        }
        
        # –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º regex –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        self.url_pattern = re.compile(r'http\S+|www\S+|https\S+')
        self.email_pattern = re.compile(r'\S*@\S*\s?')
        self.phone_pattern = re.compile(r'[\+\(\)\-\d\s]{10,}')
        self.non_russian_pattern = re.compile(r'[^–∞-—è—ë\s]')
        self.space_pattern = re.compile(r'\s+')
        self.digit_in_word_pattern = re.compile(r'\d+')  # –î–ª—è –ø–æ–∏—Å–∫–∞ —Ü–∏—Ñ—Ä –≤ —Å–ª–æ–≤–∞—Ö
    
    def fast_preprocess(self, text):
        """–°–£–ü–ï–† –ë–´–°–¢–†–ê–Ø –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π"""
        if pd.isna(text) or not text or text == '':
            return ""
        
        # –ë—ã—Å—Ç—Ä–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Å—Ç—Ä–æ–∫—É –∏ –æ—á–∏—Å—Ç–∫–∞
        text = str(text).lower().strip()
        
        if not text:  # –ü–æ—Å–ª–µ strip –º–æ–∂–µ—Ç —Å—Ç–∞—Ç—å –ø—É—Å—Ç—ã–º
            return ""
        
        # –û–î–ù–û–í–†–ï–ú–ï–ù–ù–ê–Ø –æ—á–∏—Å—Ç–∫–∞ (–±—ã—Å—Ç—Ä–µ–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –∑–∞–º–µ–Ω)
        text = self.url_pattern.sub('', text)
        text = self.email_pattern.sub('', text)
        text = self.phone_pattern.sub('', text)
        text = self.non_russian_pattern.sub(' ', text)
        text = self.space_pattern.sub(' ', text).strip()
        
        if not text:
            return ""
        
        # –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å–ª–æ–≤
        words = []
        for word in text.split():
            word_len = len(word)
            
            # –ë–´–°–¢–†–ê–Ø –ø—Ä–æ–≤–µ—Ä–∫–∞ —É—Å–ª–æ–≤–∏–π (—Å–∞–º—ã–µ —á–∞—Å—Ç—ã–µ —Å–ª—É—á–∞–∏ —Å–Ω–∞—á–∞–ª–∞)
            if word_len < 3 or word_len > 25:
                continue
                
            if word in self.stop_words:
                continue
                
            if word.isdigit():
                continue
                
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–∏—Ñ—Ä –≤ —Å–ª–æ–≤–µ (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
            if self.digit_in_word_pattern.search(word):
                continue
                
            words.append(word)
        
        return ' '.join(words)
    
    def train_model(self, texts: List[str], labels: List[int]):
        """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ"""
        start_time = time.time()
        print("–ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
        
        # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
        processed_texts = []
        for i, text in enumerate(texts):
            processed = self.fast_preprocess(text)
            if processed:  # –¢–æ–ª—å–∫–æ –Ω–µ–ø—É—Å—Ç—ã–µ —Ç–µ–∫—Å—Ç—ã
                processed_texts.append(processed)
        
        # –ï—Å–ª–∏ –ø–æ—Å–ª–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—Å—Ç–∞–ª–∏—Å—å –ø—É—Å—Ç—ã–µ —Ç–µ–∫—Å—Ç—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—ã
        if len(processed_texts) < len(texts):
            print(f"–ü–æ—Å–ª–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—Å—Ç–∞–ª–æ—Å—å {len(processed_texts)}/{len(texts)} —Ç–µ–∫—Å—Ç–æ–≤")
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –¥–ª—è –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è
            for i, text in enumerate(texts):
                if not processed_texts or i >= len(processed_texts):
                    processed_texts.append(self.fast_preprocess(text) or " ")
        
        print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(processed_texts)} —Ç–µ–∫—Å—Ç–æ–≤")
        
        # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
        X = self.vectorizer.fit_transform(processed_texts)
        y = np.array(labels)
        
        print(f"üìä –°–æ–∑–¥–∞–Ω–æ {X.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        
        # –ë—ã—Å—Ç—Ä–∞—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
        cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
        
        # –¢–æ–ª—å–∫–æ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å
        model = LogisticRegression(
            C=1.0,
            class_weight='balanced',
            max_iter=500,  # –ú–µ–Ω—å—à–µ –∏—Ç–µ—Ä–∞—Ü–∏–π
            random_state=42,
            solver='lbfgs',
            multi_class='multinomial'
        )
        
        # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
        scores = cross_val_score(model, X, y, cv=cv, scoring='f1_macro')
        mean_f1 = np.mean(scores)
        
        print(f"üìä Cross-val Macro-F1: {mean_f1:.4f} (+/- {np.std(scores):.4f})")
        
        # –û–±—É—á–µ–Ω–∏–µ
        self.model = model
        self.model.fit(X, y)
        self.is_trained = True
        self.best_f1 = mean_f1
        
        # –ë—ã—Å—Ç—Ä–∞—è –æ—Ü–µ–Ω–∫–∞
        y_pred = self.model.predict(X)
        final_f1 = f1_score(y, y_pred, average='macro')
        
        training_time = time.time() - start_time
        
        print(f"–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {training_time:.2f} —Å–µ–∫")
        print(f"Final Macro-F1: {final_f1:.4f}")
        
        return {
            "best_model": "logistic_regression",
            "cross_val_f1": float(mean_f1),
            "final_f1": float(final_f1),
            "training_time": training_time
        }
    
    def predict(self, texts: List[str]):
        """–°–≤–µ—Ä—Ö–±—ã—Å—Ç—Ä–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ"""
        if not self.is_trained:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞")
        
        # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
        processed_texts = [self.fast_preprocess(text) or " " for text in texts]
        
        X = self.vectorizer.transform(processed_texts)
        probabilities = self.model.predict_proba(X)
        predictions = self.model.predict(X)
        confidence_scores = np.max(probabilities, axis=1)
        
        return predictions, confidence_scores, probabilities

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å
analyzer = OptimizedSentimentAnalyzer()

def load_and_train_model():
    """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—É—á–µ–Ω–∏–µ"""
    try:
        if not os.path.exists('train.csv'):
            print("–§–∞–π–ª train.csv –Ω–µ –Ω–∞–π–¥–µ–Ω!")
            return False
        
        print("üìÅ –ó–∞–≥—Ä—É–∂–∞–µ–º train.csv...")
        df = pd.read_csv('train.csv')
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–ª–æ–Ω–æ–∫
        required_cols = ['ID', 'text', 'src', 'label']
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            print(f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–æ–ª–æ–Ω–∫–∏: {missing_cols}")
            return False
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö (–∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º src, –∫–∞–∫ –ø—Ä–æ—Å–∏–ª–∏)
        texts = df['text'].fillna('').astype(str).tolist()
        labels = df['label'].astype(int).tolist()
        
        # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –º–µ—Ç–æ–∫
        valid_labels = {0, 1, 2}
        if not all(label in valid_labels for label in labels):
            print("‚ùå –ù–∞–π–¥–µ–Ω—ã –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–µ –º–µ—Ç–∫–∏")
            return False
        
        print(f"üì¶ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(texts)} samples")
        
        # –û–±—É—á–µ–Ω–∏–µ
        metrics = analyzer.train_model(texts, labels)
        
        print("üéâ –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ!")
        return True
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        return False

@app.post("/api/analyze")
async def analyze_csv(file: UploadFile = File(...)):
    """–ê–Ω–∞–ª–∏–∑ CSV —Ñ–∞–π–ª–æ–≤"""
    try:
        if not analyzer.is_trained:
            raise HTTPException(400, "–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞")
        
        start_time = time.time()
        contents = await file.read()
        
        # –ë—ã—Å—Ç—Ä–æ–µ —á—Ç–µ–Ω–∏–µ
        df = None
        for encoding in ['utf-8', 'cp1251', 'windows-1251']:
            try:
                df = pd.read_csv(io.StringIO(contents.decode(encoding)))
                break
            except:
                continue
        
        if df is None:
            raise HTTPException(400, "–ù–µ —É–¥–∞–ª–æ—Å—å –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å —Ñ–∞–π–ª")
        
        # –ü–æ–∏—Å–∫ —Ç–µ–∫—Å—Ç–æ–≤–æ–π –∫–æ–ª–æ–Ω–∫–∏
        text_column = None
        for col in df.columns:
            if any(keyword in col.lower() for keyword in ['text', '—Ç–µ–∫—Å—Ç', 'review', '–æ—Ç–∑—ã–≤']):
                text_column = col
                break
        
        if not text_column:
            text_column = df.columns[0]
        
        # –î–æ–±–∞–≤–ª—è–µ–º ID –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if 'ID' not in df.columns:
            df['ID'] = [str(uuid.uuid4())[:8] for _ in range(len(df))]
        
        texts = df[text_column].fillna('').astype(str).tolist()
        
        # –ë—ã—Å—Ç—Ä–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        predictions, confidence_scores, probabilities = analyzer.predict(texts)
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
        results = []
        for i, (text, pred, conf) in enumerate(zip(texts, predictions, confidence_scores)):
            results.append({
                'ID': str(df['ID'].iloc[i]),
                'text': text[:80] + '...' if len(text) > 80 else text,
                'sentiment': int(pred),
                'confidence': float(conf)
            })
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è –¥–∏–∞–≥—Ä–∞–º–º—ã
        sentiment_counts = {
            'neutral': int((predictions == 0).sum()),
            'positive': int((predictions == 1).sum()),
            'negative': int((predictions == 2).sum())
        }
        
        total = len(predictions)
        sentiment_percentages = {
            'neutral': round(sentiment_counts['neutral'] / total * 100, 1) if total > 0 else 0,
            'positive': round(sentiment_counts['positive'] / total * 100, 1) if total > 0 else 0,
            'negative': round(sentiment_counts['negative'] / total * 100, 1) if total > 0 else 0
        }
        
        # –ò—Ç–æ–≥–æ–≤—ã–π CSV
        result_df = pd.DataFrame([{'ID': r['ID'], 'sentiment': r['sentiment']} for r in results])
        output = io.StringIO()
        result_df.to_csv(output, index=False)
        csv_content = output.getvalue()
        
        processing_time = time.time() - start_time
        
        return JSONResponse({
            "success": True,
            "processing_time_seconds": round(processing_time, 2),
            "statistics": {
                "total_samples": len(df),
                "sentiment_distribution": sentiment_counts,
                "sentiment_percentages": sentiment_percentages,
                "confidence_avg": float(np.mean(confidence_scores))
            },
            "preview": results[:8],
            "results_csv": csv_content
        })
        
    except Exception as e:
        raise HTTPException(500, f"–û—à–∏–±–∫–∞: {str(e)}")

@app.get("/api/health")
async def health_check():
    return {
        "status": "healthy",
        "model_trained": analyzer.is_trained,
        "macro_f1_score": analyzer.best_f1 if analyzer.is_trained else None
    }

@app.on_event("startup")
async def startup_event():
    print("üöÄ –ó–∞–ø—É—Å–∫ API...")
    if load_and_train_model():
        print("–°–µ—Ä–≤–µ—Ä –≥–æ—Ç–æ–≤!")
    else:
        print("–û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)